{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import early_stopping, log_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    gc.collect()\n",
    "    print('--------Loading original data from file: {}--------'.format(file_path))\n",
    "    raw_data = pd.read_csv(file_path)\n",
    "    print('--------Data loaded successfully---------')\n",
    "    return raw_data\n",
    "\n",
    "def get_target(data):\n",
    "    target_col = 'target'\n",
    "    conditions = [\n",
    "            data[\"click_bool\"] == 1,\n",
    "            data[\"booking_bool\"] == 1,\n",
    "        ]\n",
    "    choices = [1, 5]\n",
    "    data[target_col] = np.select(conditions, choices, default=0)   \n",
    "    return data\n",
    "    \n",
    "\n",
    "def merge_similar_data(data):\n",
    "    comp_rate = data.filter(regex=r'^comp[12345678]_rate\\d*$').mean(axis=1)\n",
    "    comp_inv = data.filter(regex=r'^comp[12345678]_inv\\d*$').mean(axis=1)\n",
    "    comp_rate_percent_diff = data.filter(regex=r'^comp[12345678]_rate_percent_diff\\d*$').mean(axis=1)\n",
    "    data['comp_rate'] = comp_rate\n",
    "    data['comp_inv'] = comp_inv\n",
    "    data['comp_rate_percent_diff'] = comp_rate_percent_diff\n",
    "    data.drop(labels = data.filter(regex=r'^comp[1-8]').columns, axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def drop_low_value_attributes(data):\n",
    "    sparse_col = data.isnull().mean()[data.isnull().mean() > 0.1].index.to_list()\n",
    "    # In the Point-biserial correlation coefficient calculation, the valuable_col shows a positive correlation or statistically significant with 'click_bool'. \n",
    "    # Raw and balanced data are used to calculate the correlation coefficient('click_bool' '1' : '0' == 50 : 2; '1' : '0' == 1 : 1).\n",
    "    valuable_col = ['prop_review_score', 'prop_location_score2', 'srch_query_affinity_score', 'comp_rate', 'comp_inv']\n",
    "    low_value_col = ['visitor_hist_starrating', 'visitor_hist_adr_usd', 'orig_destination_distance', 'comp_rate_percent_diff', 'gross_bookings_usd']\n",
    "    low_value_col = [c for c in low_value_col if c in data.columns.values]\n",
    "    data = data.drop(labels = low_value_col, axis = 1)\n",
    "    return data\n",
    "\n",
    "def add_date_features(data, datetime_key = 'date_time'):\n",
    "    dates = pd.to_datetime(data[datetime_key])\n",
    "    data['month'] = dates.dt.month\n",
    "    data['quarter'] = dates.dt.quarter\n",
    "    data = data.drop(labels=datetime_key, axis = 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_mean_position_features(data):\n",
    "    prop_mean_position = data[data['random_bool'] == 0]\n",
    "    prop_mean_position = prop_mean_position.groupby([\"srch_destination_id\", \"prop_id\"]).agg({\"position\": \"mean\"})\n",
    "    prop_mean_position = prop_mean_position.rename(columns = {\"position\": \"mean_position\"}).reset_index()\n",
    "    prop_mean_position['mean_position'] = prop_mean_position['mean_position'].astype(int) \n",
    "    data = data.merge(prop_mean_position, how = 'left', on = [\"srch_destination_id\", \"prop_id\"])\n",
    "\n",
    "    # data['mean_position'] = 1 / data['mean_position']\n",
    "    # data['position'] = 1 / data['position']\n",
    "    return data\n",
    "\n",
    "def add_diff_features(data, groupby_key, target_col):\n",
    "    average_value_by_srch_id = data.groupby(groupby_key)[target_col].transform('mean')\n",
    "    data[f'{target_col}_diff'] = data[target_col] - average_value_by_srch_id\n",
    "    return data\n",
    "\n",
    "def add_prop_pop_score(data):\n",
    "    # 按 id 列分组，并计算 'click' 和 'book' 列的总和\n",
    "    grouped = data.groupby('prop_id')[['click_bool', 'booking_bool']].sum()\n",
    "    # 计算 'click' 和 'book' 列的总和，并乘以相应的权重\n",
    "    grouped['weighted_sum'] = (grouped['booking_bool'] * 5 + grouped['click_bool'] * 1)\n",
    "    # 计算每个 'id' 出现的次数\n",
    "    id_counts = data['prop_id'].value_counts()\n",
    "    id_counts.sort_index(inplace=True)\n",
    "    # 将加权总和除以每个 'id' 出现的次数\n",
    "    grouped['pop_score'] = grouped['weighted_sum'] / id_counts\n",
    "    data['pop_score'] = data['prop_id'].map(grouped['pop_score'])\n",
    "    return data\n",
    "\n",
    "def normalize_features(input_df, group_key, target_column, take_log10=False):\n",
    "    # for numerical stability\n",
    "    epsilon = 1e-4\n",
    "    if take_log10:\n",
    "        input_df[target_column] = np.log10(input_df[target_column] + epsilon)\n",
    "    methods = [\"mean\", \"std\"]\n",
    "\n",
    "    df = input_df.groupby(group_key).agg({target_column: methods})\n",
    "\n",
    "    df.columns = df.columns.droplevel()\n",
    "    col = {}\n",
    "    for method in methods:\n",
    "        col[method] = target_column + \"_\" + method\n",
    "\n",
    "    df.rename(columns=col, inplace=True)\n",
    "    df_merge = input_df.merge(df.reset_index(), on=group_key)\n",
    "    df_merge[target_column + \"_norm_by_\" + group_key] = (\n",
    "        df_merge[target_column] - df_merge[target_column + \"_mean\"]\n",
    "    ) / df_merge[target_column + \"_std\"]\n",
    "    df_merge = df_merge.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n",
    "\n",
    "    gc.collect()\n",
    "    return df_merge\n",
    "\n",
    "def normalization(input_df):\n",
    "    input_df = normalize_features(input_df, group_key=\"srch_id\", target_column=\"price_usd\", take_log10=True)\n",
    "    input_df = normalize_features(input_df, group_key=\"prop_id\", target_column=\"price_usd\")\n",
    "    input_df = normalize_features(input_df, group_key=\"srch_id\", target_column=\"prop_starrating\")\n",
    "    input_df = normalize_features(input_df, group_key=\"srch_id\", target_column=\"prop_location_score2\")\n",
    "    input_df = normalize_features(input_df, group_key=\"srch_id\", target_column=\"prop_location_score1\")\n",
    "    input_df = normalize_features(input_df, group_key=\"srch_id\", target_column=\"prop_review_score\")\n",
    "    return input_df\n",
    "    \n",
    "\n",
    "def train_data_preprocessing(data, kind='train'):\n",
    "    print('--------Preprocessing training data--------')\n",
    "    # 第一步 合并同类数据\n",
    "    data = merge_similar_data(data)\n",
    "    # 第二步 删除缺失值过多的列 （如果缺失值过多且与'click_bool'相关性弱，则删除）\n",
    "    data = drop_low_value_attributes(data)\n",
    "    print('--------Low value cols have been dropped---------')\n",
    "    \n",
    "    # feature engineering\n",
    "    # 第一步 提取日期特征\n",
    "    data = add_date_features(data)\n",
    "    # if kind == 'train': \n",
    "    #     data = add_mean_position_features(data)\n",
    "\n",
    "    # people will prefer to choose the cheaper/higher score props in each search\n",
    "    # add new features based on expected visitor’s behavior\n",
    "    prop_attr = ['price_usd', 'prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2']\n",
    "    # 第二步 In each search people will tend to click or even book hotels that have:higher star ratings, higher reviews,better located\n",
    "    for col in prop_attr:\n",
    "        data = add_diff_features(data, 'srch_id', col)\n",
    "    # add pop_score based on the percentage of booking and clicking\n",
    "\n",
    "    # 第三步 加入平均位置和popularity，这个部分处理有点复杂，version1中的代码暂时被注释掉了，到时我加上就会\n",
    "    # if kind == 'train':\n",
    "    #     data = add_prop_pop_score(data)\n",
    "    \n",
    "    # 第三步 根据clik，book计算得到目标列0，1，5，数值越大权重越高\n",
    "    if kind == 'train':\n",
    "        data = get_target(data)\n",
    "\n",
    "    # 第三步 均值化\n",
    "    data = normalization(data)\n",
    "\n",
    "    # 按\"srch_id\"排序 与模型训练所需得group顺序保持一致\n",
    "    data = data.sort_values(\"srch_id\")\n",
    "\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # use pop_score to replace 'click_bool' and 'booking_bool', and drop the original columns\n",
    "\n",
    "    # 删掉test数据集没有的click和book\n",
    "    data = data.drop(labels=['click_bool', 'booking_bool'], axis=1)\n",
    "    print('--------Data Preprocessing Completed--------')\n",
    "    return data\n",
    "\n",
    "def get_categorical_index(df):\n",
    "    categorical_features = [\n",
    "        \"month\",\n",
    "        \"quarter\",\n",
    "        \"prop_country_id\",\n",
    "        \"site_id\",\n",
    "        \"visitor_location_country_id\",\n",
    "    ]\n",
    "    categorical_features = [c for c in categorical_features if c in df.columns.values]\n",
    "    categorical_features_index = [df.columns.get_loc(x) for x in categorical_features]\n",
    "    return categorical_features_index\n",
    "\n",
    "def remove_columns(x1, ignore_column=[\"srch_id\", \"prop_id\", \"position\", \"random_bool\"]):\n",
    "    ignore_column = [c for c in ignore_column if c in x1.columns.values]\n",
    "    x1 = x1.drop(labels=ignore_column, axis=1)\n",
    "    return x1\n",
    "\n",
    "def split_train_data(data_for_training):\n",
    "    sample = data_for_training.copy()\n",
    "\n",
    "    x1 = sample.iloc[:int(len(sample) * 0.8)]\n",
    "    x2 = sample.iloc[int(len(sample) * 0.8):]\n",
    "    y1 = x1['target'].values\n",
    "    y2 = x2['target'].values\n",
    "    x1 = x1.drop(labels='target', axis=1)\n",
    "    x2 = x2.drop(labels='target', axis=1)\n",
    "\n",
    "    groups = x1[\"srch_id\"].value_counts(sort=False).sort_index()\n",
    "    eval_groups = x2[\"srch_id\"].value_counts(sort=False).sort_index()\n",
    "    len(eval_groups), len(x2), len(x1), len(groups)\n",
    "\n",
    "    x1 = remove_columns(x1)\n",
    "    x2 = remove_columns(x2)\n",
    "\n",
    "    return (x1, x2, y1, y2, groups, eval_groups)\n",
    "\n",
    "def train_model(\n",
    "    x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model=None\n",
    "):\n",
    "    if not name_of_model:\n",
    "        name_of_model = str(int(time.time()))\n",
    "\n",
    "    categorical_features_index = get_categorical_index(x1)\n",
    "    clf = lightgbm.LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        n_estimators=5000,\n",
    "        learning_rate=lr,\n",
    "        random_state=69,\n",
    "        seed=69,\n",
    "        # boosting=method,\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training on train set with columns: {}\".format(x1.columns.values))\n",
    "    early_stopping_callback = early_stopping(stopping_rounds=200, verbose=True)\n",
    "    log_evaluation_callback = log_evaluation(period=20)\n",
    "    \n",
    "    clf.fit(\n",
    "        x1,\n",
    "        y1,\n",
    "        eval_set=[(x1, y1), (x2, y2)],\n",
    "        eval_group=[groups, eval_groups],\n",
    "        group=groups,\n",
    "        eval_at=38,\n",
    "        callbacks=[early_stopping_callback, log_evaluation_callback],\n",
    "        categorical_feature=categorical_features_index,\n",
    "    )\n",
    "    gc.collect()\n",
    "    pickle.dump(clf, open(os.path.join(output_dir, \"model.dat\"), \"wb\"))\n",
    "    return clf\n",
    "\n",
    "def predict(name_of_model, test_data, output_dir):\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    model = pickle.load(open(os.path.join(output_dir, \"model.dat\"), \"rb\"))\n",
    "\n",
    "\n",
    "    test_data_srch_id_prop_id = test_data[[\"srch_id\", \"prop_id\"]]\n",
    "\n",
    "    test_data = remove_columns(test_data)\n",
    "\n",
    "    categorical_features_numbers = get_categorical_index(test_data)\n",
    "\n",
    "    print(\"Predicting on train set with columns: {}\".format(test_data.columns.values))\n",
    "    kwargs = {}\n",
    "    kwargs = {\"categorical_feature\": categorical_features_numbers}\n",
    "\n",
    "    predictions = model.predict(test_data, **kwargs)\n",
    "    test_data_srch_id_prop_id[\"prediction\"] = predictions\n",
    "    del test_data\n",
    "    gc.collect()\n",
    "\n",
    "    test_data_srch_id_prop_id = test_data_srch_id_prop_id.sort_values(\n",
    "        [\"srch_id\", \"prediction\"], ascending=False\n",
    "    )\n",
    "    print(\"Saving predictions into submission.csv\")\n",
    "    test_data_srch_id_prop_id[[\"srch_id\", \"prop_id\"]].to_csv(\n",
    "        os.path.join(output_dir, \"submission.csv\"), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('training_set_VU_DM.csv')\n",
    "test_data = pd.read_csv('test_set_VU_DM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Preprocessing training data--------\n",
      "--------Low value cols have been dropped---------\n",
      "--------Data Preprocessing Completed--------\n"
     ]
    }
   ],
   "source": [
    "train = train_data_preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, y1, y2, groups, eval_groups = split_train_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on train set with columns: ['site_id' 'visitor_location_country_id' 'prop_country_id'\n",
      " 'prop_starrating' 'prop_review_score' 'prop_brand_bool'\n",
      " 'prop_location_score1' 'prop_location_score2' 'prop_log_historical_price'\n",
      " 'price_usd' 'promotion_flag' 'srch_destination_id' 'srch_length_of_stay'\n",
      " 'srch_booking_window' 'srch_adults_count' 'srch_children_count'\n",
      " 'srch_room_count' 'srch_saturday_night_bool' 'srch_query_affinity_score'\n",
      " 'comp_rate' 'comp_inv' 'month' 'quarter' 'price_usd_diff'\n",
      " 'prop_starrating_diff' 'prop_review_score_diff'\n",
      " 'prop_location_score1_diff' 'prop_location_score2_diff'\n",
      " 'price_usd_norm_by_srch_id' 'price_usd_norm_by_prop_id'\n",
      " 'prop_starrating_norm_by_srch_id' 'prop_location_score2_norm_by_srch_id'\n",
      " 'prop_location_score1_norm_by_srch_id'\n",
      " 'prop_review_score_norm_by_srch_id']\n",
      "[LightGBM] [Warning] seed is set=69, random_state=69 will be ignored. Current value: seed=69\n",
      "[LightGBM] [Warning] seed is set=69, random_state=69 will be ignored. Current value: seed=69\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5003\n",
      "[LightGBM] [Info] Number of data points in the train set: 3966677, number of used features: 32\n",
      "[LightGBM] [Warning] seed is set=69, random_state=69 will be ignored. Current value: seed=69\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[20]\ttraining's ndcg@38: 0.512071\tvalid_1's ndcg@38: 0.509213\n",
      "[40]\ttraining's ndcg@38: 0.5192\tvalid_1's ndcg@38: 0.512607\n",
      "[60]\ttraining's ndcg@38: 0.52441\tvalid_1's ndcg@38: 0.514695\n",
      "[80]\ttraining's ndcg@38: 0.529122\tvalid_1's ndcg@38: 0.516701\n",
      "[100]\ttraining's ndcg@38: 0.533129\tvalid_1's ndcg@38: 0.518114\n",
      "[120]\ttraining's ndcg@38: 0.536413\tvalid_1's ndcg@38: 0.519099\n",
      "[140]\ttraining's ndcg@38: 0.53909\tvalid_1's ndcg@38: 0.519198\n",
      "[160]\ttraining's ndcg@38: 0.541494\tvalid_1's ndcg@38: 0.519284\n",
      "[180]\ttraining's ndcg@38: 0.543856\tvalid_1's ndcg@38: 0.519688\n",
      "[200]\ttraining's ndcg@38: 0.54594\tvalid_1's ndcg@38: 0.519828\n",
      "[220]\ttraining's ndcg@38: 0.547915\tvalid_1's ndcg@38: 0.520341\n",
      "[240]\ttraining's ndcg@38: 0.549722\tvalid_1's ndcg@38: 0.520603\n",
      "[260]\ttraining's ndcg@38: 0.551652\tvalid_1's ndcg@38: 0.520626\n",
      "[280]\ttraining's ndcg@38: 0.553144\tvalid_1's ndcg@38: 0.520366\n",
      "[300]\ttraining's ndcg@38: 0.554734\tvalid_1's ndcg@38: 0.520441\n",
      "[320]\ttraining's ndcg@38: 0.556301\tvalid_1's ndcg@38: 0.52025\n",
      "[340]\ttraining's ndcg@38: 0.558054\tvalid_1's ndcg@38: 0.520322\n",
      "[360]\ttraining's ndcg@38: 0.559618\tvalid_1's ndcg@38: 0.52025\n",
      "[380]\ttraining's ndcg@38: 0.560832\tvalid_1's ndcg@38: 0.520284\n",
      "[400]\ttraining's ndcg@38: 0.562071\tvalid_1's ndcg@38: 0.520326\n",
      "[420]\ttraining's ndcg@38: 0.563375\tvalid_1's ndcg@38: 0.520305\n",
      "[440]\ttraining's ndcg@38: 0.564656\tvalid_1's ndcg@38: 0.52033\n",
      "Early stopping, best iteration is:\n",
      "[246]\ttraining's ndcg@38: 0.550322\tvalid_1's ndcg@38: 0.520675\n"
     ]
    }
   ],
   "source": [
    "model = train_model(x1, x2, y1, y2, groups, eval_groups, 0.12, \"dart\", \"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
